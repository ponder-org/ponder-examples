{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b117c8-51b2-4a7c-acac-5db439945586",
   "metadata": {},
   "source": [
    "# Tutorial 3: Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511071e4-0121-45fc-8fe0-6574609f61b5",
   "metadata": {},
   "source": [
    "## ``read_sql:``Working with existing tables\n",
    "\n",
    "To work with data stored in an existing table in Snowflake, we use the ``read_sql`` command and provide the name of the table ``CUSTOMER`` and pass in ``auto`` to the connection parameter to auto-populate the connection information based on what we provided earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c3f33a-c621-4bf1-b2c2-01b05531fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"YELLOW_TRIPDATA_O\", con='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8c4d0e-c38b-4f62-a87f-8155ac869dfc",
   "metadata": {},
   "source": [
    "Now that we have a Ponder DataFrame that points to the ``CUSTOMER`` table in your data warehouse, you can now work on your DataFrame ``df`` just like you would typically do with any pandas dataframe – with all the computation happening on your warehouse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a73e8-54e8-4ff8-bdcd-6ade394e33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e41980-4ecc-4b0a-abd2-14ddd66c6724",
   "metadata": {},
   "source": [
    "## ``read_csv:`` Working with CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb3e0e-4ed0-4b7b-a62f-2512689b6a5f",
   "metadata": {},
   "source": [
    "### Working with remote CSV files\n",
    "To work with ``CSV`` files, use the ``read_csv`` command to feed in the filepath to the CSV file. If the filepath is a remote path to the CSV (e.g., filepath to S3, GCS, or a public dataset URL), you can enter the path directly as follow. Ponder will automatically process your CSV file and load it into a temporary table in your data warehouse account for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c727d98-c1e2-4065-8243-dc723c908a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ponder-org/ponder-datasets/main/yellow_tripdata_2015-01.csv\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96659862-0482-440f-a433-a81270e92d50",
   "metadata": {},
   "source": [
    "Now that your data is loaded into a temporary table in your data warehouse and Ponder DataFrame is pointing to the table, you can now work on your DataFrame ``df`` just like you would typically do with any pandas dataframe – with all the computation happening on your warehouse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d68d5ff-8b4c-424d-a761-8afe347276fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ad8d30-a5bc-48f4-ae39-e15743bc6742",
   "metadata": {},
   "source": [
    "### Working with your own local CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceb50df-7f41-48b1-9cf7-08e1173346f0",
   "metadata": {},
   "source": [
    "If you have a CSV file locally that you want to analyze with Ponder, we provide an interface that allows you to stage the file for analysis.\n",
    "\n",
    "1- **Uploading to Ponder:** If you have a CSV file on your local machine, you must first upload them through the notebook interface. You can upload files to your Jupyter directory using the file upload functionality provided by Jupyter notebook.\n",
    "\n",
    "2- **Staging CSV file to a remote path and reading it using ``read_csv()``:** After uploading your files to the Jupyter directory, you will need to stage the file to a remote path so that it is accessible by read_csv, as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d863b9cd-40ba-4a81-8a7d-8f9c0b5819c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Teleporter()\n",
    "t.depulso(\"movies.csv\")\n",
    "remote_path = t.teleported_path(\"movies.csv\")\n",
    "df = pd.read_csv(remote_path, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39dcd92-5748-414f-b5ed-a63ecc2aa02d",
   "metadata": {},
   "source": [
    "# Tutorial 3: Write Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325dc155-050b-4650-9772-545c4e984e77",
   "metadata": {},
   "source": [
    "You can use ``to_sql()`` command to write records stored in a DataFrame to a SQL database. Tables can be newly created, appended to, or overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee82d24-6cd5-42e4-9ef6-bc4fd3885c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read a csv file\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ponder-org/ponder-datasets/main/yellow_tripdata_2015-01.csv\", header=0)\n",
    "\n",
    "#transpose your dataframe\n",
    "df_transposed = df.T\n",
    "\n",
    "#write your data into a table in your database.\n",
    "\n",
    "df_transposed.to_sql(\"TRANSPOSED_DATA\",con=snowflake_con, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c66eba-a566-45d0-b334-5609a71ba88f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
