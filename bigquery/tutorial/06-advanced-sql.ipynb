{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10931462-726a-45b0-b335-01f942a03a46",
   "metadata": {},
   "source": [
    "# Tutorial 6: Advanced Usage - Working with SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42201643-37bd-472c-b3f7-ba10cfeb309a",
   "metadata": {},
   "source": [
    "In this tutorial, we will showcase a few tips and tricks that helps you more easily move between Ponder and SQL. We will be using the [MIMIC-III demo dataset](https://physionet.org/content/mimiciii-demo/1.4/) as an example dataset. The MIMIC-III Clinical Database contains deidentified health-related data of patients who stayed in an intensive care unit (ICU) at the Beth Israel Deaconess Medical Center in Boston. The demo dataset contains records for 100 patients across three tables `PATIENTS`, `ICUSTAYS`, and `ADMISSIONS`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13d4e70-9c07-4d8c-ae20-64a1fc60115f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Definition (DDL) with SQLAlchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119561b-0c30-45a6-8527-8d3a97dd1d5a",
   "metadata": {},
   "source": [
    "In SQL, DDL statements involve modifications to the database schema, e.g., `CREATE`, `ALTER`, `DROP`. Oftentimes, you may want to run a DDL statement alongside your analysis, either via an external query editor or through SQLAlchemy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b09b76-6230-42a0-bd92-6955f2532201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SQLAlchemy if you don't have it already\n",
    "! pip install --upgrade bigquery-sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b74b11-940d-4950-8f77-ff4c416551ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os; os.chdir(\"..\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c4782-f733-4837-8d41-76a967aedec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import credential\n",
    "\n",
    "from bigquery.sqlalchemy import URL\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(URL(\n",
    "    account = credential.params[\"account\"],\n",
    "    user = credential.params[\"user\"],\n",
    "    password = credential.params[\"password\"],\n",
    "    database = credential.params[\"database\"],\n",
    "    schema = credential.params[\"schema\"],\n",
    "    warehouse = credential.params[\"warehouse\"],\n",
    "    role=credential.params[\"role\"],\n",
    "))\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3020f9-9694-4af2-9063-cddd19266ca0",
   "metadata": {},
   "source": [
    "Then we can run the SQL query directly to create a new database named `MIMIC3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5d670a-a171-47e0-9491-a6072acc2d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "connection.execute(\"CREATE DATABASE IF NOT EXISTS MIMIC3;\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3bde16-fe9f-4160-99b4-a4d56a1e7009",
   "metadata": {},
   "source": [
    "Now if we print out the database, we can see the new table MIMIC3 added: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c656e5-b145-446b-a4ab-3065e60c4ce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ret = connection.execute(\"SHOW DATABASES;\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1fb52b-32af-4901-9534-94edde038786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "pandas.DataFrame(ret.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500047be-bb6a-496a-83cc-c169a58f4194",
   "metadata": {},
   "source": [
    "### Existing SQL DML with Ponder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2481eb90-3934-443c-9c4e-c7996b875f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16afc971-155e-4efd-8175-9c2c0940122c",
   "metadata": {},
   "source": [
    "We will be using a few example tables for the remainder of this tutorial. You can run this python script to populate the required datasets to your database. This will populate three different tables `PATIENTS`, `ADMISSIONS`, and `ICUSTAYS` to your database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e222b37f-ca2a-4ef7-b7c3-d0b795eb5942",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python populate_mimic3.py > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19310b11-3fe3-46b3-8f03-9e939ff4a2c1",
   "metadata": {},
   "source": [
    "Oftentime, you may already have an existing SQL script that you've been using to join and denormalize some tables or perform some pre-aggregation or ETL before you perform your analysis. You want to reuse that SQL code while working with Ponder for the remaining analysis workflow. In this example, we show how you can feed this into the `pd.read_sql` to operate on the resulting table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488f3d7b-c470-4349-b41f-5dd1a96122e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import credential\n",
    "import ponder.bigquery\n",
    "import modin.pandas as pd\n",
    "credential.params[\"database\"] = \"MIMIC3\"\n",
    "bigquery_con = ponder.bigquery.connect(user=credential.params[\"user\"],password=credential.params[\"password\"],account=credential.params[\"account\"],role=credential.params[\"role\"],database=credential.params[\"database\"],schema=credential.params[\"schema\"],warehouse=credential.params[\"warehouse\"])\n",
    "ponder.bigquery.init(bigquery_con,enable_ssl=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846673b7-00bc-454d-b84b-4f87319edca5",
   "metadata": {},
   "source": [
    "For example, we may want to use [this existing SQL query](https://mimic.mit.edu/docs/iii/tutorials/intro-to-mimic-iii/#5-patient-age-and-mortality) from the MIT MIMIC-III tutorial to jumpstart our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51691fd9-958d-4a6c-8ba7-76386892727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('''SELECT p.\"subject_id\", p.\"dob\", a.\"hadm_id\",\n",
    "                    a.\"admittime\", p.\"expire_flag\"\n",
    "                    FROM MIMIC3.PUBLIC.ADMISSIONS as a\n",
    "                    INNER JOIN MIMIC3.PUBLIC.PATIENTS as p\n",
    "                    ON p.\"subject_id\" = a.\"subject_id\"''', con = bigquery_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377809f1-8759-43be-9621-7b0516df077d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e022f1-8bbf-4f97-b265-173415c00f6f",
   "metadata": {},
   "source": [
    "Then we can continue using Ponder by writing pandas as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880e9c7-0ac8-4208-b9ea-18e9c9990e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"dob\"] = pd.to_datetime(df[\"dob\"])\n",
    "df[\"admittime\"] = pd.to_datetime(df[\"admittime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e01754-4685-418e-8d62-927914bfe894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"age\"] = df[\"admittime\"].dt.year  - df[\"dob\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9efb8f-3f08-4c1f-a5a6-7ad76806d142",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6d7a31-9713-4932-b08e-26f6f4867601",
   "metadata": {},
   "source": [
    "### Working with multiple tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626f2c71-3a51-4e88-b38e-45b6c9197469",
   "metadata": {},
   "source": [
    "With Ponder, you can work with multiple tables at the same time by creating different dataframes using the `read_sql` or `read_csv` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f6b2e2-4e65-4aeb-9c66-e8656af36304",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = pd.read_sql(\"PATIENTS\", con=bigquery_con)\n",
    "admissions = pd.read_sql(\"ADMISSIONS\", con=bigquery_con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc6eb01-50fd-4b3c-bb99-b87f220e4915",
   "metadata": {},
   "source": [
    "Now we can work with these two dataframes in pandas. Here, we perform the same query as the SQL query above: \n",
    "```sql\n",
    "SELECT p.\"subject_id\", p.\"dob\", a.\"hadm_id\",\n",
    "        a.\"admittime\", p.\"expire_flag\"\n",
    "        FROM MIMIC3.PUBLIC.ADMISSIONS as a\n",
    "        INNER JOIN MIMIC3.PUBLIC.PATIENTS as p\n",
    "        ON p.\"subject_id\" = a.\"subject_id\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5620ba-ab16-4bcd-803d-023aa50e731e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patients.merge(admissions,on=\"subject_id\")[[\"subject_id_x\", \"dob\", \"hadm_id\",\"admittime\", \"expire_flag\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
