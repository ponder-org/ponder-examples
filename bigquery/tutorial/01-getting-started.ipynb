{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57c6b83b-d5e8-47f6-81b9-d2c4761ae9e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tutorial 1: Getting Started\n",
    "\n",
    "In this tutorial, we will walk through how you can get started running pandas on BigQuery using Ponder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1619cf-c5c4-48b6-bcef-bb40820dd8ea",
   "metadata": {},
   "source": [
    "### BigQuery Connections Credential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9d5d76-4861-4c35-816a-9ba02cfe5f6e",
   "metadata": {},
   "source": [
    "To run Ponder on your data warehouse, you must first establish database connection to your warehouse. \n",
    "Please edit the `credentials.py` file to populate the connections information, we will be using the same connections information throughout the tutorial series. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326cd98c-304c-42f4-9504-4cb4c31d5fc5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Note: </b> <span> If can not find the BigQuery account information you need to set up your database connection, please follow our <a href=\"https://docs.ponder.io/resources/BigQueryInfo.html\">step-by-step guide</a> here for more information. </spam></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d4f04a-113b-4374-a509-2758f1f96129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.chdir(\"..\")\n",
    "import credential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbee2f6-ad70-4b3e-a0cf-3d7bae33264f",
   "metadata": {},
   "source": [
    "### Uploading Example Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d896b-2c19-4b7c-97bf-426041a31d63",
   "metadata": {},
   "source": [
    "We will be using a few example datasets for the tutorial. You can run this python script to populate the required datasets to your database. This will add three different tables to your database populated with example datasets: \n",
    "- [PONDER_TAXI](https://raw.githubusercontent.com/ponder-org/ponder-datasets/main/yellow_tripdata_2015-01.csv)\n",
    "- [PONDER_CITIBIKE](https://raw.githubusercontent.com/ponder-org/ponder-datasets/main/citibike_trial.csv)\n",
    "- [PONDER_BOOK](https://github.com/ponder-org/ponder-datasets/blob/main/books.csv).\n",
    "\n",
    "Note that you only need to run the following script once for the tables to get populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da0037e-d2fa-4943-bc86-8835bd56dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python populate_datasets.py > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed59fe47-7e6a-49e5-ab5d-be9f08e9704b",
   "metadata": {},
   "source": [
    "### Connecting to BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e111fd-481d-4d9b-b3de-65c6f654e5e0",
   "metadata": {},
   "source": [
    "Ponder uses your data warehouse as an engine, so we need to establish a connection with BigQuery in order to start querying the data. The code below shows how you can configure the database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74c49b4-999a-4db4-8c91-8ebab230c2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ponder.bigquery\n",
    "\n",
    "# Create a Ponder BigQuery Connections object\n",
    "bigquery_con = ponder.bigquery.connect(\n",
    "    user=credential.params[\"user\"],\n",
    "    password=credential.params[\"password\"],\n",
    "    account=credential.params[\"account\"],\n",
    "    role=credential.params[\"role\"],\n",
    "    database=credential.params[\"database\"],\n",
    "    schema=credential.params[\"schema\"],\n",
    "    warehouse=credential.params[\"warehouse\"]\n",
    ")\n",
    "# Initialize the BigQuery connection\n",
    "ponder.bigquery.init(bigquery_con,enable_ssl=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1befaf78-c3fc-463a-becc-190c64e7ff73",
   "metadata": {},
   "source": [
    "If you have succesfully established the connection, you should see the following output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9eccd7be-c688-44ce-b724-676c319d29dd",
   "metadata": {},
   "source": [
    "Connected to\n",
    "       ___               __\n",
    "      / _ \\___  ___  ___/ /__ ____\n",
    "     / ___/ _ \\/ _ \\/ _  / -_) __/\n",
    "    /_/___\\___/_//_/\\_,_/\\__/_/\n",
    "      / __/__ _____  _____ ____\n",
    "     _\\ \\/ -_) __/ |/ / -_) __/\n",
    "    /___/\\__/_/  |___/\\__/_/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0274eb-36b8-47ea-9110-bbd802d0643e",
   "metadata": {},
   "source": [
    "### Starting Pondering ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82a1849-d53c-4c63-a31e-747445fabec2",
   "metadata": {},
   "source": [
    "Now that we have the connection initialized. Let's read the **PONDER_BOOKS** table that already exists in your database. This dataset comes from the [Goodreads dataset from Kaggle](https://www.kaggle.com/datasets/jealousleopard/goodreadsbooks) and contains a books and their review information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8797cd9c-e2b0-4589-8f60-8583e1347b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7737dd-03ae-4b6f-aeaf-378609b9d5d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"PONDER_BOOKS\", bigquery_con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a74a49-45b5-4070-ace6-5b36f6edadad",
   "metadata": {},
   "source": [
    "Let's first print out the dataframe and take a look at the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fab392c-75d4-4a3b-b0b7-f16f72205ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb9f61c-798c-44c0-b3a3-02b0ccab8d10",
   "metadata": {},
   "source": [
    "Now we can start hacking away with pandas! Note that every single operations you are doing here with pandas is directly being run on BigQuery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c0962d-4af0-4eb0-84ed-7f9acc04bee3",
   "metadata": {},
   "source": [
    "First, let's take a look at the basic statistics around the numerical columns in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf838cd-a015-4878-91c0-2f1b96bee621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aecf38-8090-4b74-b7e3-a4964a6e5410",
   "metadata": {},
   "source": [
    "Let's say we want to normalize the numerical columns by doing a standard z-score normalization (where $\\mu$ is the mean and $\\sigma$ is the standard deviation). \n",
    "\n",
    "$$ x' = \\frac{x-\\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a028cf8e-c887-4ee4-aabb-66e5b80ea1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.select_dtypes(include='number').columns\n",
    "(df[x] - df[x].mean())/df[x].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83575259-23e6-4d80-bb0f-ec43b2a7d35d",
   "metadata": {},
   "source": [
    "Next, let's look at all the columns that are non-numerical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458322f6-d045-44e5-8880-218c1b179764",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include='object').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb131dc-8ded-4fe6-8294-383a8b4e729d",
   "metadata": {},
   "source": [
    "We can look at the number of distinct value in each of these non-numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1797fd3-0523-47b2-8c36-aec684d6d173",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include='object').nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782c2dda-9f85-4852-9555-b0018a658388",
   "metadata": {},
   "source": [
    "We see that there are 27 different languages represented by `language_code` in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b438d8-f1ee-419d-8665-2b94a3c3deaf",
   "metadata": {},
   "source": [
    "To feed this into a machine learning model, we want to [one-hot encode](https://en.wikipedia.org/wiki/One-hot) this catagorical column to a set of binary features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e817ea-5b64-4ea6-bd1b-21e8b73d92bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = pd.get_dummies(df, columns=\"language_code\")\n",
    "encoded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb61a1d-7453-4996-bba1-d503027f2b98",
   "metadata": {},
   "source": [
    "We select out only the columns with names matching \"language\". This leaves us with all the converted binary columns, which is often referred to as the indicator matrix. This can be an input to a machine learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acd2087-4127-47b6-8d8b-c3182c5700ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_matrix= encoded_df.filter(regex=\"language\")\n",
    "indicator_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d6204-cb95-479b-8979-72a215d49fc2",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ef8db-786d-4e92-89b8-0034dcd6cb2c",
   "metadata": {},
   "source": [
    "In this tutorial, we saw how you can get started in running common data science operations in pandas directly on the `PONDER_BOOK` table in your BigQuery.\n",
    "\n",
    "That means that every single operation that you performed in this tutorial is being executed directly in your data warehouse! The only data that is being pulled out of the warehouse is the few lines of results that is printed in the notebook!\n",
    "\n",
    "Note that if you were to write the equivalent SQL query to run these pandas commands on BigQuery, it would take many lines of code to express the same query. If you're interested in learning about why, check out this [blogpost](https://ponder.io/pandas-vs-sql-part-2-pandas-is-more-concise/#:~:text=the%20window%20function.-,Conclusion,and%20dropping%20sparsely%20populated%20features.).\n",
    "\n",
    "In our next tutorial, we will share more details on how Ponder works and how you can leverage Ponder to scale up your data science workflow!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
